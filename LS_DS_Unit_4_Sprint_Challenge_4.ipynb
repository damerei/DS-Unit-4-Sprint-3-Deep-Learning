{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LS_DS_Unit_4_Sprint_Challenge_4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "xhU3R-8dzk5z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Lambda School Data Science Unit 4 Sprint Challenge 4\n",
        "\n",
        "## RNNs, CNNs, AutoML, and more...\n",
        "\n",
        "In this sprint challenge, you'll explore some of the cutting edge of Data Science.\n",
        "\n",
        "*Caution* - these approaches can be pretty heavy computationally. All problems were designed so that you should be able to achieve results within at most 5-10 minutes of runtime on Colab or a comparable environment. If something is running longer, doublecheck your approach!"
      ]
    },
    {
      "metadata": {
        "id": "-5UwGRnJOmD4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 1 - RNNs\n",
        "\n",
        "Use an RNN to fit a simple classification model on tweets to distinguish from tweets from Austen Allred and tweets from Weird Al Yankovic.\n",
        "\n",
        "Following is code to scrape the needed data (no API auth needed, uses [twitterscraper](https://github.com/taspinar/twitterscraper)):"
      ]
    },
    {
      "metadata": {
        "id": "3if1yTMUoG3U",
        "colab_type": "code",
        "outputId": "6075f571-d8b1-4e91-bfb9-26ad7a467261",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install twitterscraper"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting twitterscraper\n",
            "  Downloading https://files.pythonhosted.org/packages/38/7d/0bf84247b78d7d223914cbf410e1160203a65d39086aaf8c6cad521cec74/twitterscraper-0.9.3.tar.gz\n",
            "Collecting coala-utils~=0.5.0 (from twitterscraper)\n",
            "  Downloading https://files.pythonhosted.org/packages/54/00/74ec750cfc4e830f9d1cfdd4d559f3d2d4ba1b834b78d5266446db3fd1d6/coala_utils-0.5.1-py3-none-any.whl\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.6/dist-packages (from twitterscraper) (0.0.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from twitterscraper) (4.2.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from twitterscraper) (2.18.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from bs4->twitterscraper) (4.6.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->twitterscraper) (3.0.4)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->twitterscraper) (2.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->twitterscraper) (2019.3.9)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->twitterscraper) (1.22)\n",
            "Building wheels for collected packages: twitterscraper\n",
            "  Building wheel for twitterscraper (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/45/50/9b/70128bca07e2bf8b5ed3f504002e9e74a6eaa5e756341b6931\n",
            "Successfully built twitterscraper\n",
            "Installing collected packages: coala-utils, twitterscraper\n",
            "Successfully installed coala-utils-0.5.1 twitterscraper-0.9.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DS-9ksWjoJit",
        "colab_type": "code",
        "outputId": "0c3512e4-5cd4-4dc6-9cda-baf00c835f59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1114
        }
      },
      "cell_type": "code",
      "source": [
        "from twitterscraper import query_tweets\n",
        "\n",
        "austen_tweets = query_tweets('from:austen', 1000)\n",
        "len(austen_tweets)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO: queries: ['from:austen since:2006-03-21 until:2006-11-14', 'from:austen since:2006-11-14 until:2007-07-11', 'from:austen since:2007-07-11 until:2008-03-05', 'from:austen since:2008-03-05 until:2008-10-30', 'from:austen since:2008-10-30 until:2009-06-25', 'from:austen since:2009-06-25 until:2010-02-19', 'from:austen since:2010-02-19 until:2010-10-15', 'from:austen since:2010-10-15 until:2011-06-11', 'from:austen since:2011-06-11 until:2012-02-04', 'from:austen since:2012-02-04 until:2012-09-30', 'from:austen since:2012-09-30 until:2013-05-26', 'from:austen since:2013-05-26 until:2014-01-20', 'from:austen since:2014-01-20 until:2014-09-15', 'from:austen since:2014-09-15 until:2015-05-12', 'from:austen since:2015-05-12 until:2016-01-05', 'from:austen since:2016-01-05 until:2016-08-31', 'from:austen since:2016-08-31 until:2017-04-26', 'from:austen since:2017-04-26 until:2017-12-21', 'from:austen since:2017-12-21 until:2018-08-16', 'from:austen since:2018-08-16 until:2019-04-12']\n",
            "INFO: Querying from:austen since:2007-07-11 until:2008-03-05\n",
            "INFO: Querying from:austen since:2006-03-21 until:2006-11-14\n",
            "INFO: Querying from:austen since:2009-06-25 until:2010-02-19\n",
            "INFO: Querying from:austen since:2010-02-19 until:2010-10-15\n",
            "INFO: Querying from:austen since:2008-10-30 until:2009-06-25\n",
            "INFO: Querying from:austen since:2011-06-11 until:2012-02-04\n",
            "INFO: Querying from:austen since:2010-10-15 until:2011-06-11\n",
            "INFO: Querying from:austen since:2008-03-05 until:2008-10-30\n",
            "INFO: Querying from:austen since:2012-09-30 until:2013-05-26\n",
            "INFO: Querying from:austen since:2014-01-20 until:2014-09-15\n",
            "INFO: Querying from:austen since:2013-05-26 until:2014-01-20\n",
            "INFO: Querying from:austen since:2017-12-21 until:2018-08-16\n",
            "INFO: Querying from:austen since:2017-04-26 until:2017-12-21\n",
            "INFO: Querying from:austen since:2015-05-12 until:2016-01-05\n",
            "INFO: Querying from:austen since:2012-02-04 until:2012-09-30\n",
            "INFO: Querying from:austen since:2016-01-05 until:2016-08-31\n",
            "INFO: Querying from:austen since:2018-08-16 until:2019-04-12\n",
            "INFO: Querying from:austen since:2016-08-31 until:2017-04-26\n",
            "INFO: Querying from:austen since:2014-09-15 until:2015-05-12\n",
            "INFO: Querying from:austen since:2006-11-14 until:2007-07-11\n",
            "INFO: Got 0 tweets for from%3Aausten%20since%3A2006-03-21%20until%3A2006-11-14.\n",
            "INFO: Got 0 tweets (0 new).\n",
            "INFO: Got 0 tweets for from%3Aausten%20since%3A2016-08-31%20until%3A2017-04-26.\n",
            "INFO: Got 0 tweets (0 new).\n",
            "INFO: Got 0 tweets for from%3Aausten%20since%3A2011-06-11%20until%3A2012-02-04.\n",
            "INFO: Got 0 tweets (0 new).\n",
            "INFO: Got 0 tweets for from%3Aausten%20since%3A2009-06-25%20until%3A2010-02-19.\n",
            "INFO: Got 0 tweets (0 new).\n",
            "INFO: Got 0 tweets for from%3Aausten%20since%3A2008-10-30%20until%3A2009-06-25.\n",
            "INFO: Got 0 tweets (0 new).\n",
            "INFO: Got 0 tweets for from%3Aausten%20since%3A2010-02-19%20until%3A2010-10-15.\n",
            "INFO: Got 0 tweets (0 new).\n",
            "INFO: Got 0 tweets for from%3Aausten%20since%3A2014-01-20%20until%3A2014-09-15.\n",
            "INFO: Got 0 tweets (0 new).\n",
            "INFO: Got 0 tweets for from%3Aausten%20since%3A2015-05-12%20until%3A2016-01-05.\n",
            "INFO: Got 0 tweets for from%3Aausten%20since%3A2006-11-14%20until%3A2007-07-11.\n",
            "INFO: Got 0 tweets for from%3Aausten%20since%3A2008-03-05%20until%3A2008-10-30.\n",
            "INFO: Got 0 tweets (0 new).\n",
            "INFO: Got 0 tweets (0 new).\n",
            "INFO: Got 0 tweets (0 new).\n",
            "INFO: Got 0 tweets for from%3Aausten%20since%3A2014-09-15%20until%3A2015-05-12.\n",
            "INFO: Got 0 tweets (0 new).\n",
            "INFO: Got 0 tweets for from%3Aausten%20since%3A2012-09-30%20until%3A2013-05-26.\n",
            "INFO: Got 0 tweets (0 new).\n",
            "INFO: Got 0 tweets for from%3Aausten%20since%3A2016-01-05%20until%3A2016-08-31.\n",
            "INFO: Got 0 tweets (0 new).\n",
            "INFO: Got 0 tweets for from%3Aausten%20since%3A2007-07-11%20until%3A2008-03-05.\n",
            "INFO: Got 0 tweets (0 new).\n",
            "INFO: Got 0 tweets for from%3Aausten%20since%3A2010-10-15%20until%3A2011-06-11.\n",
            "INFO: Got 0 tweets for from%3Aausten%20since%3A2013-05-26%20until%3A2014-01-20.\n",
            "INFO: Got 0 tweets (0 new).\n",
            "INFO: Got 0 tweets (0 new).\n",
            "INFO: Got 1 tweets for from%3Aausten%20since%3A2012-02-04%20until%3A2012-09-30.\n",
            "INFO: Got 1 tweets (1 new).\n",
            "INFO: Got 60 tweets for from%3Aausten%20since%3A2017-04-26%20until%3A2017-12-21.\n",
            "INFO: Got 61 tweets (60 new).\n",
            "INFO: Got 60 tweets for from%3Aausten%20since%3A2018-08-16%20until%3A2019-04-12.\n",
            "INFO: Got 121 tweets (60 new).\n",
            "INFO: Got 60 tweets for from%3Aausten%20since%3A2017-12-21%20until%3A2018-08-16.\n",
            "INFO: Got 181 tweets (60 new).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "181"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "metadata": {
        "id": "fLKqFh8DovaN",
        "colab_type": "code",
        "outputId": "64b0d621-7e74-4181-9116-406e8c518465",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "austen_tweets[0].text"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I love love love working with great people.pic.twitter.com/fCKOm6Vl'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "MRQeIIf1orCS",
        "colab_type": "code",
        "outputId": "44b57b5e-2a0e-4656-ca06-d77637caf593",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1114
        }
      },
      "cell_type": "code",
      "source": [
        "al_tweets = query_tweets('from:AlYankovic', 1000)\n",
        "len(al_tweets)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO: queries: ['from:AlYankovic since:2006-03-21 until:2006-11-14', 'from:AlYankovic since:2006-11-14 until:2007-07-11', 'from:AlYankovic since:2007-07-11 until:2008-03-05', 'from:AlYankovic since:2008-03-05 until:2008-10-30', 'from:AlYankovic since:2008-10-30 until:2009-06-25', 'from:AlYankovic since:2009-06-25 until:2010-02-19', 'from:AlYankovic since:2010-02-19 until:2010-10-15', 'from:AlYankovic since:2010-10-15 until:2011-06-11', 'from:AlYankovic since:2011-06-11 until:2012-02-04', 'from:AlYankovic since:2012-02-04 until:2012-09-30', 'from:AlYankovic since:2012-09-30 until:2013-05-26', 'from:AlYankovic since:2013-05-26 until:2014-01-20', 'from:AlYankovic since:2014-01-20 until:2014-09-15', 'from:AlYankovic since:2014-09-15 until:2015-05-12', 'from:AlYankovic since:2015-05-12 until:2016-01-05', 'from:AlYankovic since:2016-01-05 until:2016-08-31', 'from:AlYankovic since:2016-08-31 until:2017-04-26', 'from:AlYankovic since:2017-04-26 until:2017-12-21', 'from:AlYankovic since:2017-12-21 until:2018-08-16', 'from:AlYankovic since:2018-08-16 until:2019-04-12']\n",
            "INFO: Querying from:AlYankovic since:2006-11-14 until:2007-07-11\n",
            "INFO: Querying from:AlYankovic since:2014-09-15 until:2015-05-12\n",
            "INFO: Querying from:AlYankovic since:2012-02-04 until:2012-09-30\n",
            "INFO: Querying from:AlYankovic since:2011-06-11 until:2012-02-04\n",
            "INFO: Querying from:AlYankovic since:2017-12-21 until:2018-08-16\n",
            "INFO: Querying from:AlYankovic since:2012-09-30 until:2013-05-26\n",
            "INFO: Querying from:AlYankovic since:2016-01-05 until:2016-08-31\n",
            "INFO: Querying from:AlYankovic since:2015-05-12 until:2016-01-05\n",
            "INFO: Querying from:AlYankovic since:2014-01-20 until:2014-09-15\n",
            "INFO: Querying from:AlYankovic since:2018-08-16 until:2019-04-12\n",
            "INFO: Querying from:AlYankovic since:2010-02-19 until:2010-10-15\n",
            "INFO: Querying from:AlYankovic since:2008-03-05 until:2008-10-30\n",
            "INFO: Querying from:AlYankovic since:2010-10-15 until:2011-06-11\n",
            "INFO: Querying from:AlYankovic since:2006-03-21 until:2006-11-14\n",
            "INFO: Querying from:AlYankovic since:2009-06-25 until:2010-02-19\n",
            "INFO: Querying from:AlYankovic since:2008-10-30 until:2009-06-25\n",
            "INFO: Querying from:AlYankovic since:2013-05-26 until:2014-01-20\n",
            "INFO: Querying from:AlYankovic since:2017-04-26 until:2017-12-21\n",
            "INFO: Querying from:AlYankovic since:2016-08-31 until:2017-04-26\n",
            "INFO: Querying from:AlYankovic since:2007-07-11 until:2008-03-05\n",
            "INFO: Got 0 tweets for from%3AAlYankovic%20since%3A2006-11-14%20until%3A2007-07-11.\n",
            "INFO: Got 0 tweets (0 new).\n",
            "INFO: Got 0 tweets for from%3AAlYankovic%20since%3A2007-07-11%20until%3A2008-03-05.\n",
            "INFO: Got 0 tweets (0 new).\n",
            "INFO: Got 0 tweets for from%3AAlYankovic%20since%3A2006-03-21%20until%3A2006-11-14.\n",
            "INFO: Got 0 tweets (0 new).\n",
            "INFO: Got 0 tweets for from%3AAlYankovic%20since%3A2008-03-05%20until%3A2008-10-30.\n",
            "INFO: Got 0 tweets (0 new).\n",
            "INFO: Got 60 tweets for from%3AAlYankovic%20since%3A2012-09-30%20until%3A2013-05-26.\n",
            "INFO: Got 60 tweets (60 new).\n",
            "INFO: Got 60 tweets for from%3AAlYankovic%20since%3A2016-08-31%20until%3A2017-04-26.\n",
            "INFO: Got 120 tweets (60 new).\n",
            "INFO: Got 60 tweets for from%3AAlYankovic%20since%3A2009-06-25%20until%3A2010-02-19.\n",
            "INFO: Got 180 tweets (60 new).\n",
            "INFO: Got 60 tweets for from%3AAlYankovic%20since%3A2008-10-30%20until%3A2009-06-25.\n",
            "INFO: Got 240 tweets (60 new).\n",
            "INFO: Got 60 tweets for from%3AAlYankovic%20since%3A2010-02-19%20until%3A2010-10-15.\n",
            "INFO: Got 300 tweets (60 new).\n",
            "INFO: Got 60 tweets for from%3AAlYankovic%20since%3A2012-02-04%20until%3A2012-09-30.\n",
            "INFO: Got 360 tweets (60 new).\n",
            "INFO: Got 60 tweets for from%3AAlYankovic%20since%3A2011-06-11%20until%3A2012-02-04.\n",
            "INFO: Got 420 tweets (60 new).\n",
            "INFO: Got 60 tweets for from%3AAlYankovic%20since%3A2010-10-15%20until%3A2011-06-11.\n",
            "INFO: Got 480 tweets (60 new).\n",
            "INFO: Got 60 tweets for from%3AAlYankovic%20since%3A2015-05-12%20until%3A2016-01-05.\n",
            "INFO: Got 540 tweets (60 new).\n",
            "INFO: Got 60 tweets for from%3AAlYankovic%20since%3A2016-01-05%20until%3A2016-08-31.\n",
            "INFO: Got 600 tweets (60 new).\n",
            "INFO: Got 60 tweets for from%3AAlYankovic%20since%3A2013-05-26%20until%3A2014-01-20.\n",
            "INFO: Got 660 tweets (60 new).\n",
            "INFO: Got 60 tweets for from%3AAlYankovic%20since%3A2014-09-15%20until%3A2015-05-12.\n",
            "INFO: Got 720 tweets (60 new).\n",
            "INFO: Got 60 tweets for from%3AAlYankovic%20since%3A2018-08-16%20until%3A2019-04-12.\n",
            "INFO: Got 780 tweets (60 new).\n",
            "INFO: Got 60 tweets for from%3AAlYankovic%20since%3A2014-01-20%20until%3A2014-09-15.\n",
            "INFO: Got 840 tweets (60 new).\n",
            "INFO: Got 60 tweets for from%3AAlYankovic%20since%3A2017-12-21%20until%3A2018-08-16.\n",
            "INFO: Got 60 tweets for from%3AAlYankovic%20since%3A2017-04-26%20until%3A2017-12-21.\n",
            "INFO: Got 900 tweets (60 new).\n",
            "INFO: Got 960 tweets (60 new).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "960"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "id": "_dB7I87ty8f1",
        "colab_type": "code",
        "outputId": "38e7dffb-92bb-4a4c-8bbd-f1ddd951cc85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "al_tweets[0].text"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Okay, it's been long enough.  I think we're all ready for a Shirley Temple comeback.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "metadata": {
        "id": "0mrcjEu_zRl4",
        "colab_type": "code",
        "outputId": "cdea5ac9-bf26-434f-d2aa-fbc251c8ceef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "len(austen_tweets + al_tweets)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1141"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "WYCVJX6ep8iO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Your tasks:\n",
        "\n",
        "- Encode the characters to a sequence of integers for the model\n",
        "- Get the data into the appropriate shape/format, including labels and a train/test split\n",
        "- Use Keras to fit a predictive model, classifying tweets as being from Austen versus Weird Al\n",
        "- Report your overall score and accuracy\n",
        "\n",
        "For reference, the [Keras IMDB sentiment classification example](https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py) will be useful, as well the RNN code we used in class.\n",
        "\n",
        "*Note* - focus on getting a running model, not on maxing accuracy with extreme data size or epoch numbers. Only revisit and push accuracy if you get everything else done!"
      ]
    },
    {
      "metadata": {
        "id": "_QVSlFEAqWJM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO - your code!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lPn6c0x21gu1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Conclusion - RNN runs, and gives pretty decent improvement over a naive \"It's Al!\" model. To *really* improve the model, more playing with parameters, and just getting more data (particularly Austen tweets), would help. Also - RNN may well not be the best approach here, but it is at least a valid one."
      ]
    },
    {
      "metadata": {
        "id": "yz0LCZd_O4IG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 2- CNNs\n",
        "\n",
        "Time to play \"find the frog!\" Use Keras and ResNet50 to detect which of the following images contain frogs:"
      ]
    },
    {
      "metadata": {
        "id": "whIqEWR236Af",
        "colab_type": "code",
        "outputId": "7a74e30d-310d-4a3a-9ae4-5bf52d137bda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install google_images_download"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting google_images_download\n",
            "  Downloading https://files.pythonhosted.org/packages/43/51/49ebfd3a02945974b1d93e34bb96a1f9530a0dde9c2bc022b30fd658edd6/google_images_download-2.5.0.tar.gz\n",
            "Collecting selenium (from google_images_download)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n",
            "\u001b[K    100% |████████████████████████████████| 911kB 9.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium->google_images_download) (1.22)\n",
            "Building wheels for collected packages: google-images-download\n",
            "  Building wheel for google-images-download (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/d2/23/84/3cec6d566b88bef64ad727a7e805f6544b8af4a8f121f9691c\n",
            "Successfully built google-images-download\n",
            "Installing collected packages: selenium, google-images-download\n",
            "Successfully installed google-images-download-2.5.0 selenium-3.141.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EKnnnM8k38sN",
        "colab_type": "code",
        "outputId": "59f477e9-0b25-4a38-9678-af24e0176535",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "cell_type": "code",
      "source": [
        "from google_images_download import google_images_download\n",
        "\n",
        "response = google_images_download.googleimagesdownload()\n",
        "arguments = {\"keywords\": \"animal pond\", \"limit\": 5, \"print_urls\": True}\n",
        "absolute_image_paths = response.download(arguments)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Item no.: 1 --> Item name = animal pond\n",
            "Evaluating...\n",
            "Starting Download...\n",
            "Image URL: https://www.enchantedlearning.com/pgifs/Pondanimals.GIF\n",
            "Completed Image ====> 1. pondanimals.gif\n",
            "Image URL: https://i.ytimg.com/vi/NCbu0TND9vE/hqdefault.jpg\n",
            "Completed Image ====> 2. hqdefault.jpg\n",
            "Image URL: https://pklifescience.com/staticfiles/articles/images/PKLS4116_inline.png\n",
            "Completed Image ====> 3. pkls4116_inline.png\n",
            "Image URL: https://pixnio.com/free-images/fauna-animals/reptiles-and-amphibians/alligators-and-crocodiles-pictures/alligator-animal-on-pond.jpg\n",
            "Completed Image ====> 4. alligator-animal-on-pond.jpg\n",
            "Image URL: https://www.nwf.org/-/media/NEW-WEBSITE/Programs/Garden-for-Wildlife/amphibian_bronze-frog_Julia-Bartosh_400x267.ashx\n",
            "Completed Image ====> 5. amphibian_bronze-frog_julia-bartosh_400x267.ash\n",
            "\n",
            "Errors: 0\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "si5YfNqS50QU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "At time of writing at least a few do, but since the Internet changes - it is possible your 5 won't. You can easily verify yourself, and (once you have working code) increase the number of images you pull to be more sure of getting a frog. Your goal is to validly run ResNet50 on the input images - don't worry about tuning or improving the model.\n",
        "\n",
        "*Hint* - ResNet 50 doesn't just return \"frog\". The three labels it has for frogs are: `bullfrog, tree frog, tailed frog`\n",
        "\n",
        "*Stretch goal* - also check for fish."
      ]
    },
    {
      "metadata": {
        "id": "FaT07ddW3nHz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO - your code!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XEuhvSu7O5Rf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 3 - AutoML\n",
        "\n",
        "Use [TPOT](https://github.com/EpistasisLab/tpot) to fit a predictive model for the King County housing data, with `price` as the target output variable."
      ]
    },
    {
      "metadata": {
        "id": "pz0e_7Ve60pM",
        "colab_type": "code",
        "outputId": "7d7f644d-2edc-417b-bd7e-14d7044ef032",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install tpot"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tpot\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/35/a6cc358b6bb2749d6dffa1ae2427143211f3092904bbb15d1ad317ebe051/TPOT-0.9.6.tar.gz (892kB)\n",
            "\u001b[K    100% |████████████████████████████████| 901kB 18.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tpot) (1.14.6)\n",
            "Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from tpot) (1.1.0)\n",
            "Requirement already satisfied: scikit-learn>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from tpot) (0.20.3)\n",
            "Collecting deap>=1.0 (from tpot)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/29/e7f2ecbe02997b16a768baed076f5fc4781d7057cd5d9adf7c94027845ba/deap-1.2.2.tar.gz (936kB)\n",
            "\u001b[K    100% |████████████████████████████████| 942kB 18.9MB/s \n",
            "\u001b[?25hCollecting update_checker>=0.16 (from tpot)\n",
            "  Downloading https://files.pythonhosted.org/packages/17/c9/ab11855af164d03be0ff4fddd4c46a5bd44799a9ecc1770e01a669c21168/update_checker-0.16-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.6/dist-packages (from tpot) (4.28.1)\n",
            "Collecting stopit>=1.1.1 (from tpot)\n",
            "  Downloading https://files.pythonhosted.org/packages/35/58/e8bb0b0fb05baf07bbac1450c447d753da65f9701f551dca79823ce15d50/stopit-1.1.2.tar.gz\n",
            "Requirement already satisfied: pandas>=0.20.2 in /usr/local/lib/python3.6/dist-packages (from tpot) (0.22.0)\n",
            "Requirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from update_checker>=0.16->tpot) (2.18.4)\n",
            "Requirement already satisfied: python-dateutil>=2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.20.2->tpot) (2.5.3)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.20.2->tpot) (2018.9)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update_checker>=0.16->tpot) (2.6)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update_checker>=0.16->tpot) (1.22)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update_checker>=0.16->tpot) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update_checker>=0.16->tpot) (2019.3.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2->pandas>=0.20.2->tpot) (1.11.0)\n",
            "Building wheels for collected packages: tpot, deap, stopit\n",
            "  Building wheel for tpot (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/86/5c/dd/c7673fbaccb901ec1a4eb79017fa5b65766805d2a98f954b9a\n",
            "  Building wheel for deap (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/22/ea/bf/dc7c8a2262025a0ab5da9ef02282c198be88902791ca0c6658\n",
            "  Building wheel for stopit (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/3c/85/2b/2580190404636bfc63e8de3dff629c03bb795021e1983a6cc7\n",
            "Successfully built tpot deap stopit\n",
            "Installing collected packages: deap, update-checker, stopit, tpot\n",
            "Successfully installed deap-1.2.2 stopit-1.1.2 tpot-0.9.6 update-checker-0.16\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GflK25wp7jnf",
        "colab_type": "code",
        "outputId": "a3e65568-5bfa-4b33-ca10-da515d8b418d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/ryanleeallred/datasets/master/kc_house_data.csv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-04-12 05:27:55--  https://raw.githubusercontent.com/ryanleeallred/datasets/master/kc_house_data.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2515206 (2.4M) [text/plain]\n",
            "Saving to: ‘kc_house_data.csv’\n",
            "\n",
            "\rkc_house_data.csv     0%[                    ]       0  --.-KB/s               \rkc_house_data.csv   100%[===================>]   2.40M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2019-04-12 05:27:56 (24.1 MB/s) - ‘kc_house_data.csv’ saved [2515206/2515206]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7G5-Gqyg9A-V",
        "colab_type": "code",
        "outputId": "1140fe22-952d-4745-d628-d91766af65b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "cell_type": "code",
      "source": [
        "!head kc_house_data.csv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "id,date,price,bedrooms,bathrooms,sqft_living,sqft_lot,floors,waterfront,view,condition,grade,sqft_above,sqft_basement,yr_built,yr_renovated,zipcode,lat,long,sqft_living15,sqft_lot15\n",
            "\"7129300520\",\"20141013T000000\",221900,3,1,1180,5650,\"1\",0,0,3,7,1180,0,1955,0,\"98178\",47.5112,-122.257,1340,5650\n",
            "\"6414100192\",\"20141209T000000\",538000,3,2.25,2570,7242,\"2\",0,0,3,7,2170,400,1951,1991,\"98125\",47.721,-122.319,1690,7639\n",
            "\"5631500400\",\"20150225T000000\",180000,2,1,770,10000,\"1\",0,0,3,6,770,0,1933,0,\"98028\",47.7379,-122.233,2720,8062\n",
            "\"2487200875\",\"20141209T000000\",604000,4,3,1960,5000,\"1\",0,0,5,7,1050,910,1965,0,\"98136\",47.5208,-122.393,1360,5000\n",
            "\"1954400510\",\"20150218T000000\",510000,3,2,1680,8080,\"1\",0,0,3,8,1680,0,1987,0,\"98074\",47.6168,-122.045,1800,7503\n",
            "\"7237550310\",\"20140512T000000\",1.225e+006,4,4.5,5420,101930,\"1\",0,0,3,11,3890,1530,2001,0,\"98053\",47.6561,-122.005,4760,101930\n",
            "\"1321400060\",\"20140627T000000\",257500,3,2.25,1715,6819,\"2\",0,0,3,7,1715,0,1995,0,\"98003\",47.3097,-122.327,2238,6819\n",
            "\"2008000270\",\"20150115T000000\",291850,3,1.5,1060,9711,\"1\",0,0,3,7,1060,0,1963,0,\"98198\",47.4095,-122.315,1650,9711\n",
            "\"2414600126\",\"20150415T000000\",229500,3,1,1780,7470,\"1\",0,0,3,7,1050,730,1960,0,\"98146\",47.5123,-122.337,1780,8113\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KynXZjOY8hBL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As with previous questions, your goal is to run TPOT and successfully run and report error at the end.  Also, in the interest of time, feel free to choose small `generation=1` and `population_size=10` parameters so your pipeline runs efficiently and you are able to iterate and test.\n",
        "\n",
        "*Hint* - you'll have to drop and/or type coerce at least a few variables to get things working. It's fine to err on the side of dropping to get things running, as long as you still get a valid model with reasonable predictive power."
      ]
    },
    {
      "metadata": {
        "id": "BOREO8VJO7MZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TODO - your code!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "626zYgjkO7Vq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Part 4 - More..."
      ]
    },
    {
      "metadata": {
        "id": "__lDWfcUO8oo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Answer the following questions, with a target audience of a fellow Data Scientist:\n",
        "\n",
        "- What do you consider your strongest area, as a Data Scientist?\n",
        "- What area of Data Science would you most like to learn more about, and why?\n",
        "- Where do you think Data Science will be in 5 years?\n",
        "\n",
        "A few sentences per answer is fine - only elaborate if time allows."
      ]
    },
    {
      "metadata": {
        "id": "_Hoqe3mM_Mtc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Thank you for your hard work, and congratulations! You've learned a lot, and should proudly call yourself a Data Scientist."
      ]
    }
  ]
}